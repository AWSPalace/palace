diff --git a/backends/cuda-ref/ceed-cuda-ref-basis.c b/backends/cuda-ref/ceed-cuda-ref-basis.c
index 3ec0d47b..f7f07784 100644
--- a/backends/cuda-ref/ceed-cuda-ref-basis.c
+++ b/backends/cuda-ref/ceed-cuda-ref-basis.c
@@ -20,7 +20,6 @@
 //------------------------------------------------------------------------------
 int CeedBasisApply_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTransposeMode t_mode, CeedEvalMode eval_mode, CeedVector u, CeedVector v) {
   Ceed              ceed;
-  Ceed_Cuda        *ceed_Cuda;
   CeedInt           Q_1d, dim;
   const CeedInt     transpose      = t_mode == CEED_TRANSPOSE;
   const int         max_block_size = 32;
@@ -29,7 +28,6 @@ int CeedBasisApply_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTransposeMo
   CeedBasis_Cuda   *data;
 
   CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
-  CeedCallBackend(CeedGetData(ceed, &ceed_Cuda));
   CeedCallBackend(CeedBasisGetData(basis, &data));
 
   // Read vectors
@@ -38,7 +36,7 @@ int CeedBasisApply_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTransposeMo
   CeedCallBackend(CeedVectorGetArrayWrite(v, CEED_MEM_DEVICE, &d_v));
 
   // Clear v for transpose operation
-  if (t_mode == CEED_TRANSPOSE) {
+  if (transpose) {
     CeedSize length;
 
     CeedCallBackend(CeedVectorGetLength(v, &length));
@@ -95,17 +93,15 @@ int CeedBasisApply_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTransposeMo
 int CeedBasisApplyNonTensor_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTransposeMode t_mode, CeedEvalMode eval_mode, CeedVector u,
                                  CeedVector v) {
   Ceed                     ceed;
-  Ceed_Cuda               *ceed_Cuda;
   CeedInt                  num_nodes, num_qpts;
   const CeedInt            transpose       = t_mode == CEED_TRANSPOSE;
-  int                      elems_per_block = 1;
-  int                      grid            = num_elem / elems_per_block + ((num_elem / elems_per_block * elems_per_block < num_elem) ? 1 : 0);
+  const int                elems_per_block = 1;
+  const int                grid            = CeedDivUpInt(num_elem, elems_per_block);
   const CeedScalar        *d_u;
   CeedScalar              *d_v;
   CeedBasisNonTensor_Cuda *data;
 
   CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
-  CeedCallBackend(CeedGetData(ceed, &ceed_Cuda));
   CeedCallBackend(CeedBasisGetData(basis, &data));
   CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis, &num_qpts));
   CeedCallBackend(CeedBasisGetNumNodes(basis, &num_nodes));
@@ -117,7 +113,7 @@ int CeedBasisApplyNonTensor_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTr
   CeedCallBackend(CeedVectorGetArrayWrite(v, CEED_MEM_DEVICE, &d_v));
 
   // Clear v for transpose operation
-  if (t_mode == CEED_TRANSPOSE) {
+  if (transpose) {
     CeedSize length;
 
     CeedCallBackend(CeedVectorGetLength(v, &length));
@@ -127,16 +123,44 @@ int CeedBasisApplyNonTensor_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTr
   // Apply basis operation
   switch (eval_mode) {
     case CEED_EVAL_INTERP: {
-      void     *interp_args[] = {(void *)&num_elem, (void *)&transpose, &data->d_interp, &d_u, &d_v};
+      void     *interp_args[] = {(void *)&num_elem, &data->d_interp, &d_u, &d_v};
       const int block_size_x  = transpose ? num_nodes : num_qpts;
 
-      CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Interp, grid, block_size_x, 1, elems_per_block, interp_args));
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->InterpTranspose, grid, block_size_x, 1, elems_per_block, interp_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Interp, grid, block_size_x, 1, elems_per_block, interp_args));
+      }
     } break;
     case CEED_EVAL_GRAD: {
-      void     *grad_args[]  = {(void *)&num_elem, (void *)&transpose, &data->d_grad, &d_u, &d_v};
+      void     *grad_args[]  = {(void *)&num_elem, &data->d_grad, &d_u, &d_v};
       const int block_size_x = transpose ? num_nodes : num_qpts;
 
-      CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Grad, grid, block_size_x, 1, elems_per_block, grad_args));
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->DerivTranspose, grid, block_size_x, 1, elems_per_block, grad_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Deriv, grid, block_size_x, 1, elems_per_block, grad_args));
+      }
+    } break;
+    case CEED_EVAL_DIV: {
+      void     *div_args[]   = {(void *)&num_elem, &data->d_div, &d_u, &d_v};
+      const int block_size_x = transpose ? num_nodes : num_qpts;
+
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->DerivTranspose, grid, block_size_x, 1, elems_per_block, div_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Deriv, grid, block_size_x, 1, elems_per_block, div_args));
+      }
+    } break;
+    case CEED_EVAL_CURL: {
+      void     *curl_args[]  = {(void *)&num_elem, &data->d_curl, &d_u, &d_v};
+      const int block_size_x = transpose ? num_nodes : num_qpts;
+
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->DerivTranspose, grid, block_size_x, 1, elems_per_block, curl_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Deriv, grid, block_size_x, 1, elems_per_block, curl_args));
+      }
     } break;
     case CEED_EVAL_WEIGHT: {
       void *weight_args[] = {(void *)&num_elem, (void *)&data->d_q_weight, &d_v};
@@ -144,12 +168,6 @@ int CeedBasisApplyNonTensor_Cuda(CeedBasis basis, const CeedInt num_elem, CeedTr
       CeedCallBackend(CeedRunKernelDim_Cuda(ceed, data->Weight, grid, num_qpts, 1, elems_per_block, weight_args));
     } break;
     // LCOV_EXCL_START
-    // Evaluate the divergence to/from the quadrature points
-    case CEED_EVAL_DIV:
-      return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_DIV not supported");
-    // Evaluate the curl to/from the quadrature points
-    case CEED_EVAL_CURL:
-      return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_CURL not supported");
     // Take no action, BasisApply should not have been called
     case CEED_EVAL_NONE:
       return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_NONE does not make sense in this context");
@@ -194,6 +212,8 @@ static int CeedBasisDestroyNonTensor_Cuda(CeedBasis basis) {
   CeedCallCuda(ceed, cudaFree(data->d_q_weight));
   CeedCallCuda(ceed, cudaFree(data->d_interp));
   CeedCallCuda(ceed, cudaFree(data->d_grad));
+  CeedCallCuda(ceed, cudaFree(data->d_div));
+  CeedCallCuda(ceed, cudaFree(data->d_curl));
   CeedCallBackend(CeedFree(&data));
   return CEED_ERROR_SUCCESS;
 }
@@ -238,34 +258,43 @@ int CeedBasisCreateTensorH1_Cuda(CeedInt dim, CeedInt P_1d, CeedInt Q_1d, const
 
   CeedCallBackend(CeedBasisSetData(basis, data));
 
+  // Register backend functions
   CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Apply", CeedBasisApply_Cuda));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Destroy", CeedBasisDestroy_Cuda));
   return CEED_ERROR_SUCCESS;
 }
 
 //------------------------------------------------------------------------------
-// Create non-tensor
+// Create non-tensor H^1
 //------------------------------------------------------------------------------
 int CeedBasisCreateH1_Cuda(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp, const CeedScalar *grad,
                            const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis) {
   Ceed                     ceed;
   char                    *basis_kernel_path, *basis_kernel_source;
-  CeedInt                  num_comp;
-  const CeedInt            q_bytes      = num_qpts * sizeof(CeedScalar);
-  const CeedInt            interp_bytes = q_bytes * num_nodes;
-  const CeedInt            grad_bytes   = q_bytes * num_nodes * dim;
+  CeedInt                  num_comp, q_comp_interp, q_comp_grad;
+  const CeedInt            q_bytes = num_qpts * sizeof(CeedScalar);
   CeedBasisNonTensor_Cuda *data;
 
   CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
   CeedCallBackend(CeedCalloc(1, &data));
 
   // Copy basis data to GPU
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_GRAD, &q_comp_grad));
   CeedCallCuda(ceed, cudaMalloc((void **)&data->d_q_weight, q_bytes));
   CeedCallCuda(ceed, cudaMemcpy(data->d_q_weight, q_weight, q_bytes, cudaMemcpyHostToDevice));
-  CeedCallCuda(ceed, cudaMalloc((void **)&data->d_interp, interp_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(data->d_interp, interp, interp_bytes, cudaMemcpyHostToDevice));
-  CeedCallCuda(ceed, cudaMalloc((void **)&data->d_grad, grad_bytes));
-  CeedCallCuda(ceed, cudaMemcpy(data->d_grad, grad, grad_bytes, cudaMemcpyHostToDevice));
+  if (interp) {
+    const CeedInt interp_bytes = q_bytes * num_nodes * q_comp_interp;
+
+    CeedCallCuda(ceed, cudaMalloc((void **)&data->d_interp, interp_bytes));
+    CeedCallCuda(ceed, cudaMemcpy(data->d_interp, interp, interp_bytes, cudaMemcpyHostToDevice));
+  }
+  if (grad) {
+    const CeedInt grad_bytes = q_bytes * num_nodes * q_comp_grad;
+
+    CeedCallCuda(ceed, cudaMalloc((void **)&data->d_grad, grad_bytes));
+    CeedCallCuda(ceed, cudaMemcpy(data->d_grad, grad, grad_bytes, cudaMemcpyHostToDevice));
+  }
 
   // Compile basis kernels
   CeedCallBackend(CeedBasisGetNumComponents(basis, &num_comp));
@@ -273,11 +302,125 @@ int CeedBasisCreateH1_Cuda(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source -----\n");
   CeedCallBackend(CeedLoadSourceToBuffer(ceed, basis_kernel_path, &basis_kernel_source));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source Complete! -----\n");
-  CeedCallCuda(ceed, CeedCompile_Cuda(ceed, basis_kernel_source, &data->module, 4, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_DIM", dim,
-                                      "BASIS_NUM_COMP", num_comp));
-  CeedCallCuda(ceed, CeedGetKernel_Cuda(ceed, data->module, "Interp", &data->Interp));
-  CeedCallCuda(ceed, CeedGetKernel_Cuda(ceed, data->module, "Grad", &data->Grad));
-  CeedCallCuda(ceed, CeedGetKernel_Cuda(ceed, data->module, "Weight", &data->Weight));
+  CeedCallBackend(CeedCompile_Cuda(ceed, basis_kernel_source, &data->module, 5, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_Q_COMP_INTERP",
+                                   q_comp_interp, "BASIS_Q_COMP_DERIV", q_comp_grad, "BASIS_NUM_COMP", num_comp));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Interp", &data->Interp));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "InterpTranspose", &data->InterpTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Deriv", &data->Deriv));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "DerivTranspose", &data->DerivTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Weight", &data->Weight));
+  CeedCallBackend(CeedFree(&basis_kernel_path));
+  CeedCallBackend(CeedFree(&basis_kernel_source));
+
+  CeedCallBackend(CeedBasisSetData(basis, data));
+
+  // Register backend functions
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Apply", CeedBasisApplyNonTensor_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Destroy", CeedBasisDestroyNonTensor_Cuda));
+  return CEED_ERROR_SUCCESS;
+}
+
+//------------------------------------------------------------------------------
+// Create non-tensor H(div)
+//------------------------------------------------------------------------------
+int CeedBasisCreateHdiv_Cuda(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp, const CeedScalar *div,
+                             const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis) {
+  Ceed                     ceed;
+  char                    *basis_kernel_path, *basis_kernel_source;
+  CeedInt                  num_comp, q_comp_interp, q_comp_div;
+  const CeedInt            q_bytes = num_qpts * sizeof(CeedScalar);
+  CeedBasisNonTensor_Cuda *data;
+
+  CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
+  CeedCallBackend(CeedCalloc(1, &data));
+
+  // Copy basis data to GPU
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_DIV, &q_comp_div));
+  CeedCallCuda(ceed, cudaMalloc((void **)&data->d_q_weight, q_bytes));
+  CeedCallCuda(ceed, cudaMemcpy(data->d_q_weight, q_weight, q_bytes, cudaMemcpyHostToDevice));
+  if (interp) {
+    const CeedInt interp_bytes = q_bytes * num_nodes * q_comp_interp;
+
+    CeedCallCuda(ceed, cudaMalloc((void **)&data->d_interp, interp_bytes));
+    CeedCallCuda(ceed, cudaMemcpy(data->d_interp, interp, interp_bytes, cudaMemcpyHostToDevice));
+  }
+  if (div) {
+    const CeedInt div_bytes = q_bytes * num_nodes * q_comp_div;
+
+    CeedCallCuda(ceed, cudaMalloc((void **)&data->d_div, div_bytes));
+    CeedCallCuda(ceed, cudaMemcpy(data->d_div, div, div_bytes, cudaMemcpyHostToDevice));
+  }
+
+  // Compile basis kernels
+  CeedCallBackend(CeedBasisGetNumComponents(basis, &num_comp));
+  CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/cuda/cuda-ref-basis-nontensor.h", &basis_kernel_path));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source -----\n");
+  CeedCallBackend(CeedLoadSourceToBuffer(ceed, basis_kernel_path, &basis_kernel_source));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source Complete! -----\n");
+  CeedCallBackend(CeedCompile_Cuda(ceed, basis_kernel_source, &data->module, 5, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_Q_COMP_INTERP",
+                                   q_comp_interp, "BASIS_Q_COMP_DERIV", q_comp_div, "BASIS_NUM_COMP", num_comp));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Interp", &data->Interp));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "InterpTranspose", &data->InterpTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Deriv", &data->Deriv));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "DerivTranspose", &data->DerivTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Weight", &data->Weight));
+  CeedCallBackend(CeedFree(&basis_kernel_path));
+  CeedCallBackend(CeedFree(&basis_kernel_source));
+
+  CeedCallBackend(CeedBasisSetData(basis, data));
+
+  // Register backend functions
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Apply", CeedBasisApplyNonTensor_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Destroy", CeedBasisDestroyNonTensor_Cuda));
+  return CEED_ERROR_SUCCESS;
+}
+
+//------------------------------------------------------------------------------
+// Create non-tensor H(curl)
+//------------------------------------------------------------------------------
+int CeedBasisCreateHcurl_Cuda(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
+                              const CeedScalar *curl, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis) {
+  Ceed                     ceed;
+  char                    *basis_kernel_path, *basis_kernel_source;
+  CeedInt                  num_comp, q_comp_interp, q_comp_curl;
+  const CeedInt            q_bytes = num_qpts * sizeof(CeedScalar);
+  CeedBasisNonTensor_Cuda *data;
+
+  CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
+  CeedCallBackend(CeedCalloc(1, &data));
+
+  // Copy basis data to GPU
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_CURL, &q_comp_curl));
+  CeedCallCuda(ceed, cudaMalloc((void **)&data->d_q_weight, q_bytes));
+  CeedCallCuda(ceed, cudaMemcpy(data->d_q_weight, q_weight, q_bytes, cudaMemcpyHostToDevice));
+  if (interp) {
+    const CeedInt interp_bytes = q_bytes * num_nodes * q_comp_interp;
+
+    CeedCallCuda(ceed, cudaMalloc((void **)&data->d_interp, interp_bytes));
+    CeedCallCuda(ceed, cudaMemcpy(data->d_interp, interp, interp_bytes, cudaMemcpyHostToDevice));
+  }
+  if (curl) {
+    const CeedInt curl_bytes = q_bytes * num_nodes * q_comp_curl;
+
+    CeedCallCuda(ceed, cudaMalloc((void **)&data->d_curl, curl_bytes));
+    CeedCallCuda(ceed, cudaMemcpy(data->d_curl, curl, curl_bytes, cudaMemcpyHostToDevice));
+  }
+
+  // Compile basis kernels
+  CeedCallBackend(CeedBasisGetNumComponents(basis, &num_comp));
+  CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/cuda/cuda-ref-basis-nontensor.h", &basis_kernel_path));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source -----\n");
+  CeedCallBackend(CeedLoadSourceToBuffer(ceed, basis_kernel_path, &basis_kernel_source));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source Complete! -----\n");
+  CeedCallBackend(CeedCompile_Cuda(ceed, basis_kernel_source, &data->module, 5, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_Q_COMP_INTERP",
+                                   q_comp_interp, "BASIS_Q_COMP_DERIV", q_comp_curl, "BASIS_NUM_COMP", num_comp));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Interp", &data->Interp));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "InterpTranspose", &data->InterpTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Deriv", &data->Deriv));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "DerivTranspose", &data->DerivTranspose));
+  CeedCallBackend(CeedGetKernel_Cuda(ceed, data->module, "Weight", &data->Weight));
   CeedCallBackend(CeedFree(&basis_kernel_path));
   CeedCallBackend(CeedFree(&basis_kernel_source));
 
diff --git a/backends/cuda-ref/ceed-cuda-ref.c b/backends/cuda-ref/ceed-cuda-ref.c
index a008cbc4..42922fe6 100644
--- a/backends/cuda-ref/ceed-cuda-ref.c
+++ b/backends/cuda-ref/ceed-cuda-ref.c
@@ -54,6 +54,8 @@ static int CeedInit_Cuda_ref(const char *resource, Ceed ceed) {
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "VectorCreate", CeedVectorCreate_Cuda));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateTensorH1", CeedBasisCreateTensorH1_Cuda));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateH1", CeedBasisCreateH1_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateHdiv", CeedBasisCreateHdiv_Cuda));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateHcurl", CeedBasisCreateHcurl_Cuda));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "ElemRestrictionCreate", CeedElemRestrictionCreate_Cuda));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "QFunctionCreate", CeedQFunctionCreate_Cuda));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "QFunctionContextCreate", CeedQFunctionContextCreate_Cuda));
diff --git a/backends/cuda-ref/ceed-cuda-ref.h b/backends/cuda-ref/ceed-cuda-ref.h
index 309c1056..c904adb6 100644
--- a/backends/cuda-ref/ceed-cuda-ref.h
+++ b/backends/cuda-ref/ceed-cuda-ref.h
@@ -53,10 +53,14 @@ typedef struct {
 typedef struct {
   CUmodule    module;
   CUfunction  Interp;
-  CUfunction  Grad;
+  CUfunction  InterpTranspose;
+  CUfunction  Deriv;
+  CUfunction  DerivTranspose;
   CUfunction  Weight;
   CeedScalar *d_interp;
   CeedScalar *d_grad;
+  CeedScalar *d_div;
+  CeedScalar *d_curl;
   CeedScalar *d_q_weight;
 } CeedBasisNonTensor_Cuda;
 
@@ -118,9 +122,12 @@ CEED_INTERN int CeedElemRestrictionCreate_Cuda(CeedMemType mem_type, CeedCopyMod
 
 CEED_INTERN int CeedBasisCreateTensorH1_Cuda(CeedInt dim, CeedInt P_1d, CeedInt Q_1d, const CeedScalar *interp_1d, const CeedScalar *grad_1d,
                                              const CeedScalar *q_ref_1d, const CeedScalar *q_weight_1d, CeedBasis basis);
-
 CEED_INTERN int CeedBasisCreateH1_Cuda(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
                                        const CeedScalar *grad, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis);
+CEED_INTERN int CeedBasisCreateHdiv_Cuda(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
+                                         const CeedScalar *div, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis);
+CEED_INTERN int CeedBasisCreateHcurl_Cuda(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
+                                          const CeedScalar *curl, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis);
 
 CEED_INTERN int CeedQFunctionCreate_Cuda(CeedQFunction qf);
 
diff --git a/backends/hip-ref/ceed-hip-ref-basis.c b/backends/hip-ref/ceed-hip-ref-basis.c
index 4927f1e4..e854e8dc 100644
--- a/backends/hip-ref/ceed-hip-ref-basis.c
+++ b/backends/hip-ref/ceed-hip-ref-basis.c
@@ -19,7 +19,6 @@
 //------------------------------------------------------------------------------
 int CeedBasisApply_Hip(CeedBasis basis, const CeedInt num_elem, CeedTransposeMode t_mode, CeedEvalMode eval_mode, CeedVector u, CeedVector v) {
   Ceed              ceed;
-  Ceed_Hip         *ceed_Hip;
   CeedInt           Q_1d, dim;
   const CeedInt     transpose      = t_mode == CEED_TRANSPOSE;
   const int         max_block_size = 64;
@@ -28,7 +27,6 @@ int CeedBasisApply_Hip(CeedBasis basis, const CeedInt num_elem, CeedTransposeMod
   CeedBasis_Hip    *data;
 
   CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
-  CeedCallBackend(CeedGetData(ceed, &ceed_Hip));
   CeedCallBackend(CeedBasisGetData(basis, &data));
 
   // Read vectors
@@ -37,7 +35,7 @@ int CeedBasisApply_Hip(CeedBasis basis, const CeedInt num_elem, CeedTransposeMod
   CeedCallBackend(CeedVectorGetArrayWrite(v, CEED_MEM_DEVICE, &d_v));
 
   // Clear v for transpose operation
-  if (t_mode == CEED_TRANSPOSE) {
+  if (transpose) {
     CeedSize length;
 
     CeedCallBackend(CeedVectorGetLength(v, &length));
@@ -94,17 +92,15 @@ int CeedBasisApply_Hip(CeedBasis basis, const CeedInt num_elem, CeedTransposeMod
 int CeedBasisApplyNonTensor_Hip(CeedBasis basis, const CeedInt num_elem, CeedTransposeMode t_mode, CeedEvalMode eval_mode, CeedVector u,
                                 CeedVector v) {
   Ceed                    ceed;
-  Ceed_Hip               *ceed_Hip;
   CeedInt                 num_nodes, num_qpts;
   const CeedInt           transpose       = t_mode == CEED_TRANSPOSE;
-  int                     elems_per_block = 1;
-  int                     grid            = num_elem / elems_per_block + ((num_elem / elems_per_block * elems_per_block < num_elem) ? 1 : 0);
+  const int               elems_per_block = 1;
+  const int               grid            = CeedDivUpInt(num_elem, elems_per_block);
   const CeedScalar       *d_u;
   CeedScalar             *d_v;
   CeedBasisNonTensor_Hip *data;
 
   CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
-  CeedCallBackend(CeedGetData(ceed, &ceed_Hip));
   CeedCallBackend(CeedBasisGetData(basis, &data));
   CeedCallBackend(CeedBasisGetNumQuadraturePoints(basis, &num_qpts));
   CeedCallBackend(CeedBasisGetNumNodes(basis, &num_nodes));
@@ -116,7 +112,7 @@ int CeedBasisApplyNonTensor_Hip(CeedBasis basis, const CeedInt num_elem, CeedTra
   CeedCallBackend(CeedVectorGetArrayWrite(v, CEED_MEM_DEVICE, &d_v));
 
   // Clear v for transpose operation
-  if (t_mode == CEED_TRANSPOSE) {
+  if (transpose) {
     CeedSize length;
 
     CeedCallBackend(CeedVectorGetLength(v, &length));
@@ -126,16 +122,44 @@ int CeedBasisApplyNonTensor_Hip(CeedBasis basis, const CeedInt num_elem, CeedTra
   // Apply basis operation
   switch (eval_mode) {
     case CEED_EVAL_INTERP: {
-      void     *interp_args[] = {(void *)&num_elem, (void *)&transpose, &data->d_interp, &d_u, &d_v};
+      void     *interp_args[] = {(void *)&num_elem, &data->d_interp, &d_u, &d_v};
       const int block_size_x  = transpose ? num_nodes : num_qpts;
 
-      CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->Interp, grid, block_size_x, 1, elems_per_block, interp_args));
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->InterpTranspose, grid, block_size_x, 1, elems_per_block, interp_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->Interp, grid, block_size_x, 1, elems_per_block, interp_args));
+      }
     } break;
     case CEED_EVAL_GRAD: {
-      void     *grad_args[]  = {(void *)&num_elem, (void *)&transpose, &data->d_grad, &d_u, &d_v};
+      void     *grad_args[]  = {(void *)&num_elem, &data->d_grad, &d_u, &d_v};
       const int block_size_x = transpose ? num_nodes : num_qpts;
 
-      CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->Grad, grid, block_size_x, 1, elems_per_block, grad_args));
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->DerivTranspose, grid, block_size_x, 1, elems_per_block, grad_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->Deriv, grid, block_size_x, 1, elems_per_block, grad_args));
+      }
+    } break;
+    case CEED_EVAL_DIV: {
+      void     *div_args[]   = {(void *)&num_elem, &data->d_div, &d_u, &d_v};
+      const int block_size_x = transpose ? num_nodes : num_qpts;
+
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->DerivTranspose, grid, block_size_x, 1, elems_per_block, div_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->Deriv, grid, block_size_x, 1, elems_per_block, div_args));
+      }
+    } break;
+    case CEED_EVAL_CURL: {
+      void     *curl_args[]  = {(void *)&num_elem, &data->d_curl, &d_u, &d_v};
+      const int block_size_x = transpose ? num_nodes : num_qpts;
+
+      if (transpose) {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->DerivTranspose, grid, block_size_x, 1, elems_per_block, curl_args));
+      } else {
+        CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->Deriv, grid, block_size_x, 1, elems_per_block, curl_args));
+      }
     } break;
     case CEED_EVAL_WEIGHT: {
       void *weight_args[] = {(void *)&num_elem, (void *)&data->d_q_weight, &d_v};
@@ -143,12 +167,6 @@ int CeedBasisApplyNonTensor_Hip(CeedBasis basis, const CeedInt num_elem, CeedTra
       CeedCallBackend(CeedRunKernelDim_Hip(ceed, data->Weight, grid, num_qpts, 1, elems_per_block, weight_args));
     } break;
     // LCOV_EXCL_START
-    // Evaluate the divergence to/from the quadrature points
-    case CEED_EVAL_DIV:
-      return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_DIV not supported");
-    // Evaluate the curl to/from the quadrature points
-    case CEED_EVAL_CURL:
-      return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_CURL not supported");
     // Take no action, BasisApply should not have been called
     case CEED_EVAL_NONE:
       return CeedError(ceed, CEED_ERROR_BACKEND, "CEED_EVAL_NONE does not make sense in this context");
@@ -193,6 +211,8 @@ static int CeedBasisDestroyNonTensor_Hip(CeedBasis basis) {
   CeedCallHip(ceed, hipFree(data->d_q_weight));
   CeedCallHip(ceed, hipFree(data->d_interp));
   CeedCallHip(ceed, hipFree(data->d_grad));
+  CeedCallHip(ceed, hipFree(data->d_div));
+  CeedCallHip(ceed, hipFree(data->d_curl));
   CeedCallBackend(CeedFree(&data));
   return CEED_ERROR_SUCCESS;
 }
@@ -237,34 +257,43 @@ int CeedBasisCreateTensorH1_Hip(CeedInt dim, CeedInt P_1d, CeedInt Q_1d, const C
 
   CeedCallBackend(CeedBasisSetData(basis, data));
 
+  // Register backend functions
   CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Apply", CeedBasisApply_Hip));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Destroy", CeedBasisDestroy_Hip));
   return CEED_ERROR_SUCCESS;
 }
 
 //------------------------------------------------------------------------------
-// Create non-tensor
+// Create non-tensor H^1
 //------------------------------------------------------------------------------
 int CeedBasisCreateH1_Hip(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp, const CeedScalar *grad,
                           const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis) {
   Ceed                    ceed;
   char                   *basis_kernel_path, *basis_kernel_source;
-  CeedInt                 num_comp;
-  const CeedInt           q_bytes      = num_qpts * sizeof(CeedScalar);
-  const CeedInt           interp_bytes = q_bytes * num_nodes;
-  const CeedInt           grad_bytes   = q_bytes * num_nodes * dim;
+  CeedInt                 num_comp, q_comp_interp, q_comp_grad;
+  const CeedInt           q_bytes = num_qpts * sizeof(CeedScalar);
   CeedBasisNonTensor_Hip *data;
 
   CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
   CeedCallBackend(CeedCalloc(1, &data));
 
   // Copy basis data to GPU
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_GRAD, &q_comp_grad));
   CeedCallHip(ceed, hipMalloc((void **)&data->d_q_weight, q_bytes));
   CeedCallHip(ceed, hipMemcpy(data->d_q_weight, q_weight, q_bytes, hipMemcpyHostToDevice));
-  CeedCallHip(ceed, hipMalloc((void **)&data->d_interp, interp_bytes));
-  CeedCallHip(ceed, hipMemcpy(data->d_interp, interp, interp_bytes, hipMemcpyHostToDevice));
-  CeedCallHip(ceed, hipMalloc((void **)&data->d_grad, grad_bytes));
-  CeedCallHip(ceed, hipMemcpy(data->d_grad, grad, grad_bytes, hipMemcpyHostToDevice));
+  if (interp) {
+    const CeedInt interp_bytes = q_bytes * num_nodes * q_comp_interp;
+
+    CeedCallHip(ceed, hipMalloc((void **)&data->d_interp, interp_bytes));
+    CeedCallHip(ceed, hipMemcpy(data->d_interp, interp, interp_bytes, hipMemcpyHostToDevice));
+  }
+  if (grad) {
+    const CeedInt grad_bytes = q_bytes * num_nodes * q_comp_grad;
+
+    CeedCallHip(ceed, hipMalloc((void **)&data->d_grad, grad_bytes));
+    CeedCallHip(ceed, hipMemcpy(data->d_grad, grad, grad_bytes, hipMemcpyHostToDevice));
+  }
 
   // Compile basis kernels
   CeedCallBackend(CeedBasisGetNumComponents(basis, &num_comp));
@@ -272,13 +301,128 @@ int CeedBasisCreateH1_Hip(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes,
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source -----\n");
   CeedCallBackend(CeedLoadSourceToBuffer(ceed, basis_kernel_path, &basis_kernel_source));
   CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source Complete! -----\n");
-  CeedCallBackend(CeedCompile_Hip(ceed, basis_kernel_source, &data->module, 4, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_DIM", dim,
-                                  "BASIS_NUM_COMP", num_comp));
+  CeedCallBackend(CeedCompile_Hip(ceed, basis_kernel_source, &data->module, 5, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_Q_COMP_INTERP",
+                                  q_comp_interp, "BASIS_Q_COMP_DERIV", q_comp_grad, "BASIS_NUM_COMP", num_comp));
   CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Interp", &data->Interp));
-  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Grad", &data->Grad));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "InterpTranspose", &data->InterpTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Deriv", &data->Deriv));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "DerivTranspose", &data->DerivTranspose));
   CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Weight", &data->Weight));
   CeedCallBackend(CeedFree(&basis_kernel_path));
   CeedCallBackend(CeedFree(&basis_kernel_source));
+
+  CeedCallBackend(CeedBasisSetData(basis, data));
+
+  // Register backend functions
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Apply", CeedBasisApplyNonTensor_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Destroy", CeedBasisDestroyNonTensor_Hip));
+  return CEED_ERROR_SUCCESS;
+}
+
+//------------------------------------------------------------------------------
+// Create non-tensor H(div)
+//------------------------------------------------------------------------------
+int CeedBasisCreateHdiv_Hip(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp, const CeedScalar *div,
+                            const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis) {
+  Ceed                    ceed;
+  char                   *basis_kernel_path, *basis_kernel_source;
+  CeedInt                 num_comp, q_comp_interp, q_comp_div;
+  const CeedInt           q_bytes = num_qpts * sizeof(CeedScalar);
+  CeedBasisNonTensor_Hip *data;
+
+  CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
+  CeedCallBackend(CeedCalloc(1, &data));
+
+  // Copy basis data to GPU
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_DIV, &q_comp_div));
+  CeedCallHip(ceed, hipMalloc((void **)&data->d_q_weight, q_bytes));
+  CeedCallHip(ceed, hipMemcpy(data->d_q_weight, q_weight, q_bytes, hipMemcpyHostToDevice));
+  if (interp) {
+    const CeedInt interp_bytes = q_bytes * num_nodes * q_comp_interp;
+
+    CeedCallHip(ceed, hipMalloc((void **)&data->d_interp, interp_bytes));
+    CeedCallHip(ceed, hipMemcpy(data->d_interp, interp, interp_bytes, hipMemcpyHostToDevice));
+  }
+  if (div) {
+    const CeedInt div_bytes = q_bytes * num_nodes * q_comp_div;
+
+    CeedCallHip(ceed, hipMalloc((void **)&data->d_div, div_bytes));
+    CeedCallHip(ceed, hipMemcpy(data->d_div, div, div_bytes, hipMemcpyHostToDevice));
+  }
+
+  // Compile basis kernels
+  CeedCallBackend(CeedBasisGetNumComponents(basis, &num_comp));
+  CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/hip/hip-ref-basis-nontensor.h", &basis_kernel_path));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source -----\n");
+  CeedCallBackend(CeedLoadSourceToBuffer(ceed, basis_kernel_path, &basis_kernel_source));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source Complete! -----\n");
+  CeedCallBackend(CeedCompile_Hip(ceed, basis_kernel_source, &data->module, 5, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_Q_COMP_INTERP",
+                                  q_comp_interp, "BASIS_Q_COMP_DERIV", q_comp_div, "BASIS_NUM_COMP", num_comp));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Interp", &data->Interp));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "InterpTranspose", &data->InterpTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Deriv", &data->Deriv));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "DerivTranspose", &data->DerivTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Weight", &data->Weight));
+  CeedCallBackend(CeedFree(&basis_kernel_path));
+  CeedCallBackend(CeedFree(&basis_kernel_source));
+
+  CeedCallBackend(CeedBasisSetData(basis, data));
+
+  // Register backend functions
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Apply", CeedBasisApplyNonTensor_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Basis", basis, "Destroy", CeedBasisDestroyNonTensor_Hip));
+  return CEED_ERROR_SUCCESS;
+}
+
+//------------------------------------------------------------------------------
+// Create non-tensor H(curl)
+//------------------------------------------------------------------------------
+int CeedBasisCreateHcurl_Hip(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
+                             const CeedScalar *curl, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis) {
+  Ceed                    ceed;
+  char                   *basis_kernel_path, *basis_kernel_source;
+  CeedInt                 num_comp, q_comp_interp, q_comp_curl;
+  const CeedInt           q_bytes = num_qpts * sizeof(CeedScalar);
+  CeedBasisNonTensor_Hip *data;
+
+  CeedCallBackend(CeedBasisGetCeed(basis, &ceed));
+  CeedCallBackend(CeedCalloc(1, &data));
+
+  // Copy basis data to GPU
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_INTERP, &q_comp_interp));
+  CeedCallBackend(CeedBasisGetNumQuadratureComponents(basis, CEED_EVAL_CURL, &q_comp_curl));
+  CeedCallHip(ceed, hipMalloc((void **)&data->d_q_weight, q_bytes));
+  CeedCallHip(ceed, hipMemcpy(data->d_q_weight, q_weight, q_bytes, hipMemcpyHostToDevice));
+  if (interp) {
+    const CeedInt interp_bytes = q_bytes * num_nodes * q_comp_interp;
+
+    CeedCallHip(ceed, hipMalloc((void **)&data->d_interp, interp_bytes));
+    CeedCallHip(ceed, hipMemcpy(data->d_interp, interp, interp_bytes, hipMemcpyHostToDevice));
+  }
+  if (curl) {
+    const CeedInt curl_bytes = q_bytes * num_nodes * q_comp_curl;
+
+    CeedCallHip(ceed, hipMalloc((void **)&data->d_curl, curl_bytes));
+    CeedCallHip(ceed, hipMemcpy(data->d_curl, curl, curl_bytes, hipMemcpyHostToDevice));
+  }
+
+  // Compile basis kernels
+  CeedCallBackend(CeedBasisGetNumComponents(basis, &num_comp));
+  CeedCallBackend(CeedGetJitAbsolutePath(ceed, "ceed/jit-source/hip/hip-ref-basis-nontensor.h", &basis_kernel_path));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source -----\n");
+  CeedCallBackend(CeedLoadSourceToBuffer(ceed, basis_kernel_path, &basis_kernel_source));
+  CeedDebug256(ceed, CEED_DEBUG_COLOR_SUCCESS, "----- Loading Basis Kernel Source Complete! -----\n");
+  CeedCallBackend(CeedCompile_Hip(ceed, basis_kernel_source, &data->module, 5, "BASIS_Q", num_qpts, "BASIS_P", num_nodes, "BASIS_Q_COMP_INTERP",
+                                  q_comp_interp, "BASIS_Q_COMP_DERIV", q_comp_curl, "BASIS_NUM_COMP", num_comp));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Interp", &data->Interp));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "InterpTranspose", &data->InterpTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Deriv", &data->Deriv));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "DerivTranspose", &data->DerivTranspose));
+  CeedCallBackend(CeedGetKernel_Hip(ceed, data->module, "Weight", &data->Weight));
+  CeedCallBackend(CeedFree(&basis_kernel_path));
+  CeedCallBackend(CeedFree(&basis_kernel_source));
+
   CeedCallBackend(CeedBasisSetData(basis, data));
 
   // Register backend functions
diff --git a/backends/hip-ref/ceed-hip-ref.c b/backends/hip-ref/ceed-hip-ref.c
index 754c0b52..eca2f4dd 100644
--- a/backends/hip-ref/ceed-hip-ref.c
+++ b/backends/hip-ref/ceed-hip-ref.c
@@ -17,8 +17,8 @@
 //------------------------------------------------------------------------------
 // HIP preferred MemType
 //------------------------------------------------------------------------------
-static int CeedGetPreferredMemType_Hip(CeedMemType *type) {
-  *type = CEED_MEM_DEVICE;
+static int CeedGetPreferredMemType_Hip(CeedMemType *mem_type) {
+  *mem_type = CEED_MEM_DEVICE;
   return CEED_ERROR_SUCCESS;
 }
 
@@ -54,6 +54,8 @@ static int CeedInit_Hip_ref(const char *resource, Ceed ceed) {
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "VectorCreate", CeedVectorCreate_Hip));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateTensorH1", CeedBasisCreateTensorH1_Hip));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateH1", CeedBasisCreateH1_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateHdiv", CeedBasisCreateHdiv_Hip));
+  CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "BasisCreateHcurl", CeedBasisCreateHcurl_Hip));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "ElemRestrictionCreate", CeedElemRestrictionCreate_Hip));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "QFunctionCreate", CeedQFunctionCreate_Hip));
   CeedCallBackend(CeedSetBackendFunction(ceed, "Ceed", ceed, "QFunctionContextCreate", CeedQFunctionContextCreate_Hip));
diff --git a/backends/hip-ref/ceed-hip-ref.h b/backends/hip-ref/ceed-hip-ref.h
index 634bb68d..89bbc7a0 100644
--- a/backends/hip-ref/ceed-hip-ref.h
+++ b/backends/hip-ref/ceed-hip-ref.h
@@ -57,10 +57,14 @@ typedef struct {
 typedef struct {
   hipModule_t   module;
   hipFunction_t Interp;
-  hipFunction_t Grad;
+  hipFunction_t InterpTranspose;
+  hipFunction_t Deriv;
+  hipFunction_t DerivTranspose;
   hipFunction_t Weight;
   CeedScalar   *d_interp;
   CeedScalar   *d_grad;
+  CeedScalar   *d_div;
+  CeedScalar   *d_curl;
   CeedScalar   *d_q_weight;
 } CeedBasisNonTensor_Hip;
 
@@ -122,9 +126,12 @@ CEED_INTERN int CeedElemRestrictionCreate_Hip(CeedMemType mem_type, CeedCopyMode
 
 CEED_INTERN int CeedBasisCreateTensorH1_Hip(CeedInt dim, CeedInt P_1d, CeedInt Q_1d, const CeedScalar *interp_1d, const CeedScalar *grad_1d,
                                             const CeedScalar *q_ref_1d, const CeedScalar *q_weight_1d, CeedBasis basis);
-
 CEED_INTERN int CeedBasisCreateH1_Hip(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
                                       const CeedScalar *grad, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis);
+CEED_INTERN int CeedBasisCreateHdiv_Hip(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
+                                        const CeedScalar *div, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis);
+CEED_INTERN int CeedBasisCreateHcurl_Hip(CeedElemTopology topo, CeedInt dim, CeedInt num_nodes, CeedInt num_qpts, const CeedScalar *interp,
+                                         const CeedScalar *curl, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis basis);
 
 CEED_INTERN int CeedQFunctionCreate_Hip(CeedQFunction qf);
 
diff --git a/include/ceed/jit-source/cuda/cuda-ref-basis-nontensor-templates.h b/include/ceed/jit-source/cuda/cuda-ref-basis-nontensor-templates.h
new file mode 100644
index 00000000..7e6450fb
--- /dev/null
+++ b/include/ceed/jit-source/cuda/cuda-ref-basis-nontensor-templates.h
@@ -0,0 +1,67 @@
+// Copyright (c) 2017-2022, Lawrence Livermore National Security, LLC and other CEED contributors.
+// All Rights Reserved. See the top-level LICENSE and NOTICE files for details.
+//
+// SPDX-License-Identifier: BSD-2-Clause
+//
+// This file is part of CEED:  http://github.com/ceed
+
+/// @file
+/// Internal header for CUDA non-tensor product basis templates
+#ifndef CEED_CUDA_REF_BASIS_NONTENSOR_TEMPLATES_H
+#define CEED_CUDA_REF_BASIS_NONTENSOR_TEMPLATES_H
+
+#include <ceed.h>
+
+//------------------------------------------------------------------------------
+// Tensor contraction
+//------------------------------------------------------------------------------
+template <int NUM_COMP, int Q_COMP, int P, int Q>
+inline __device__ void Contract(const CeedInt elem, const CeedInt strides_elem_U, const CeedInt strides_elem_V, const CeedInt strides_comp_U,
+                                const CeedInt strides_comp_V, const CeedInt strides_q_comp_V, const CeedScalar *__restrict__ d_B,
+                                const CeedScalar *__restrict__ d_U, CeedScalar *__restrict__ d_V) {
+  const CeedInt     t_id = threadIdx.x;
+  const CeedScalar *U;
+  CeedScalar        r_V[Q_COMP];
+  // TODO load B in shared memory if blockDim.z > 1?
+
+  for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
+    // Run with Q threads
+    U = d_U + elem * strides_elem_U + comp * strides_comp_U;
+    for (CeedInt d = 0; d < Q_COMP; d++) r_V[d] = 0.0;
+    for (CeedInt i = 0; i < P; i++) {
+      const CeedScalar val = U[i];
+
+      for (CeedInt d = 0; d < Q_COMP; d++) r_V[d] += d_B[i + t_id * P + d * P * Q] * val;
+    }
+    for (CeedInt d = 0; d < Q_COMP; d++) {
+      d_V[elem * strides_elem_V + comp * strides_comp_V + d * strides_q_comp_V + t_id] = r_V[d];
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// Tensor contraction transpose
+//------------------------------------------------------------------------------
+template <int NUM_COMP, int Q_COMP, int P, int Q>
+inline __device__ void ContractTranspose(const CeedInt elem, const CeedInt strides_elem_U, const CeedInt strides_elem_V, const CeedInt strides_comp_U,
+                                         const CeedInt strides_comp_V, const CeedInt strides_q_comp_U, const CeedScalar *__restrict__ d_B,
+                                         const CeedScalar *__restrict__ d_U, CeedScalar *__restrict__ d_V) {
+  const CeedInt     t_id = threadIdx.x;
+  const CeedScalar *U;
+  CeedScalar        r_V;
+  // TODO load B in shared memory if blockDim.z > 1?
+
+  for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
+    // Run with P threads
+    r_V = 0.0;
+    for (CeedInt d = 0; d < Q_COMP; d++) {
+      U = d_U + elem * strides_elem_U + comp * strides_comp_U + d * strides_q_comp_U;
+      for (CeedInt i = 0; i < Q; i++) r_V += d_B[t_id + i * P + d * P * Q] * U[i];
+    }
+    d_V[elem * strides_elem_V + comp * strides_comp_V + t_id] = r_V;
+  }
+}
+
+//------------------------------------------------------------------------------
+
+#endif  // CEED_CUDA_REF_BASIS_NONTENSOR_TEMPLATES_H
diff --git a/include/ceed/jit-source/cuda/cuda-ref-basis-nontensor.h b/include/ceed/jit-source/cuda/cuda-ref-basis-nontensor.h
index 484d755f..4b5c7f94 100644
--- a/include/ceed/jit-source/cuda/cuda-ref-basis-nontensor.h
+++ b/include/ceed/jit-source/cuda/cuda-ref-basis-nontensor.h
@@ -12,6 +12,8 @@
 
 #include <ceed.h>
 
+#include "cuda-ref-basis-nontensor-templates.h"
+
 //------------------------------------------------------------------------------
 // Non-Tensor Basis Kernels
 //------------------------------------------------------------------------------
@@ -19,65 +21,38 @@
 //------------------------------------------------------------------------------
 // Interp
 //------------------------------------------------------------------------------
-extern "C" __global__ void Interp(const CeedInt num_elem, const CeedInt transpose, const CeedScalar *d_B, const CeedScalar *__restrict__ d_U,
+extern "C" __global__ void Interp(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
                                   CeedScalar *__restrict__ d_V) {
-  const CeedInt     t_id = threadIdx.x;
-  const CeedScalar *U;
-  CeedScalar        V;
-  // TODO load B in shared memory if blockDim.z > 1?
-
   for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
-    for (CeedInt comp = 0; comp < BASIS_NUM_COMP; comp++) {
-      if (transpose) {  // run with P threads
-        U = d_U + elem * BASIS_Q + comp * num_elem * BASIS_Q;
-        V = 0.0;
-        for (CeedInt i = 0; i < BASIS_Q; i++) V += d_B[t_id + i * BASIS_P] * U[i];
-
-        d_V[elem * BASIS_P + comp * num_elem * BASIS_P + t_id] = V;
-      } else {  // run with Q threads
-        U = d_U + elem * BASIS_P + comp * num_elem * BASIS_P;
-        V = 0.0;
-        for (CeedInt i = 0; i < BASIS_P; i++) V += d_B[i + t_id * BASIS_P] * U[i];
+    Contract<BASIS_NUM_COMP, BASIS_Q_COMP_INTERP, BASIS_P, BASIS_Q>(elem, BASIS_P, BASIS_Q, BASIS_P * num_elem, BASIS_Q * num_elem,
+                                                                    BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
+  }
+}
 
-        d_V[elem * BASIS_Q + comp * num_elem * BASIS_Q + t_id] = V;
-      }
-    }
+extern "C" __global__ void InterpTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
+                                           CeedScalar *__restrict__ d_V) {
+  for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
+    ContractTranspose<BASIS_NUM_COMP, BASIS_Q_COMP_INTERP, BASIS_P, BASIS_Q>(elem, BASIS_Q, BASIS_P, BASIS_Q * num_elem, BASIS_P * num_elem,
+                                                                             BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
   }
 }
 
 //------------------------------------------------------------------------------
-// Grad
+// Deriv
 //------------------------------------------------------------------------------
-extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose, const CeedScalar *d_G, const CeedScalar *__restrict__ d_U,
-                                CeedScalar *__restrict__ d_V) {
-  const CeedInt     t_id = threadIdx.x;
-  const CeedScalar *U;
-  // TODO load G in shared memory if blockDim.z > 1?
-
+extern "C" __global__ void Deriv(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
+                                 CeedScalar *__restrict__ d_V) {
   for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
-    for (CeedInt comp = 0; comp < BASIS_NUM_COMP; comp++) {
-      if (transpose) {  // run with P threads
-        CeedScalar V = 0.0;
-        for (CeedInt dim = 0; dim < BASIS_DIM; dim++) {
-          U = d_U + elem * BASIS_Q + comp * num_elem * BASIS_Q + dim * BASIS_NUM_COMP * num_elem * BASIS_Q;
-          for (CeedInt i = 0; i < BASIS_Q; i++) V += d_G[t_id + i * BASIS_P + dim * BASIS_P * BASIS_Q] * U[i];
-        }
-
-        d_V[elem * BASIS_P + comp * num_elem * BASIS_P + t_id] = V;
-      } else {  // run with Q threads
-        CeedScalar V[BASIS_DIM];
-        U = d_U + elem * BASIS_P + comp * num_elem * BASIS_P;
-        for (CeedInt dim = 0; dim < BASIS_DIM; dim++) V[dim] = 0.0;
-        for (CeedInt i = 0; i < BASIS_P; i++) {
-          const CeedScalar val = U[i];
-          for (CeedInt dim = 0; dim < BASIS_DIM; dim++) V[dim] += d_G[i + t_id * BASIS_P + dim * BASIS_P * BASIS_Q] * val;
-        }
+    Contract<BASIS_NUM_COMP, BASIS_Q_COMP_DERIV, BASIS_P, BASIS_Q>(elem, BASIS_P, BASIS_Q, BASIS_P * num_elem, BASIS_Q * num_elem,
+                                                                   BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
+  }
+}
 
-        for (CeedInt dim = 0; dim < BASIS_DIM; dim++) {
-          d_V[elem * BASIS_Q + comp * num_elem * BASIS_Q + dim * BASIS_NUM_COMP * num_elem * BASIS_Q + t_id] = V[dim];
-        }
-      }
-    }
+extern "C" __global__ void DerivTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
+                                          CeedScalar *__restrict__ d_V) {
+  for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
+    ContractTranspose<BASIS_NUM_COMP, BASIS_Q_COMP_DERIV, BASIS_P, BASIS_Q>(elem, BASIS_Q, BASIS_P, BASIS_Q * num_elem, BASIS_P * num_elem,
+                                                                            BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
   }
 }
 
@@ -86,8 +61,8 @@ extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose,
 //------------------------------------------------------------------------------
 extern "C" __global__ void Weight(const CeedInt num_elem, const CeedScalar *__restrict__ q_weight, CeedScalar *__restrict__ d_V) {
   const CeedInt t_id = threadIdx.x;
-
   // TODO load q_weight in shared memory if blockDim.z > 1?
+
   for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
     d_V[elem * BASIS_Q + t_id] = q_weight[t_id];
   }
diff --git a/include/ceed/jit-source/hip/hip-ref-basis-nontensor-templates.h b/include/ceed/jit-source/hip/hip-ref-basis-nontensor-templates.h
new file mode 100644
index 00000000..4e568369
--- /dev/null
+++ b/include/ceed/jit-source/hip/hip-ref-basis-nontensor-templates.h
@@ -0,0 +1,67 @@
+// Copyright (c) 2017-2022, Lawrence Livermore National Security, LLC and other CEED contributors.
+// All Rights Reserved. See the top-level LICENSE and NOTICE files for details.
+//
+// SPDX-License-Identifier: BSD-2-Clause
+//
+// This file is part of CEED:  http://github.com/ceed
+
+/// @file
+/// Internal header for HIP non-tensor product basis templates
+#ifndef CEED_HIP_REF_BASIS_NONTENSOR_TEMPLATES_H
+#define CEED_HIP_REF_BASIS_NONTENSOR_TEMPLATES_H
+
+#include <ceed.h>
+
+//------------------------------------------------------------------------------
+// Tensor contraction
+//------------------------------------------------------------------------------
+template <int NUM_COMP, int Q_COMP, int P, int Q>
+inline __device__ void Contract(const CeedInt elem, const CeedInt strides_elem_U, const CeedInt strides_elem_V, const CeedInt strides_comp_U,
+                                const CeedInt strides_comp_V, const CeedInt strides_q_comp_V, const CeedScalar *__restrict__ d_B,
+                                const CeedScalar *__restrict__ d_U, CeedScalar *__restrict__ d_V) {
+  const CeedInt     t_id = threadIdx.x;
+  const CeedScalar *U;
+  CeedScalar        r_V[Q_COMP];
+  // TODO load B in shared memory if blockDim.z > 1?
+
+  for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
+    // Run with Q threads
+    U = d_U + elem * strides_elem_U + comp * strides_comp_U;
+    for (CeedInt d = 0; d < Q_COMP; d++) r_V[d] = 0.0;
+    for (CeedInt i = 0; i < P; i++) {
+      const CeedScalar val = U[i];
+
+      for (CeedInt d = 0; d < Q_COMP; d++) r_V[d] += d_B[i + t_id * P + d * P * Q] * val;
+    }
+    for (CeedInt d = 0; d < Q_COMP; d++) {
+      d_V[elem * strides_elem_V + comp * strides_comp_V + d * strides_q_comp_V + t_id] = r_V[d];
+    }
+  }
+}
+
+//------------------------------------------------------------------------------
+// Tensor contraction transpose
+//------------------------------------------------------------------------------
+template <int NUM_COMP, int Q_COMP, int P, int Q>
+inline __device__ void ContractTranspose(const CeedInt elem, const CeedInt strides_elem_U, const CeedInt strides_elem_V, const CeedInt strides_comp_U,
+                                         const CeedInt strides_comp_V, const CeedInt strides_q_comp_U, const CeedScalar *__restrict__ d_B,
+                                         const CeedScalar *__restrict__ d_U, CeedScalar *__restrict__ d_V) {
+  const CeedInt     t_id = threadIdx.x;
+  const CeedScalar *U;
+  CeedScalar        r_V;
+  // TODO load B in shared memory if blockDim.z > 1?
+
+  for (CeedInt comp = 0; comp < NUM_COMP; comp++) {
+    // Run with P threads
+    r_V = 0.0;
+    for (CeedInt d = 0; d < Q_COMP; d++) {
+      U = d_U + elem * strides_elem_U + comp * strides_comp_U + d * strides_q_comp_U;
+      for (CeedInt i = 0; i < Q; i++) r_V += d_B[t_id + i * P + d * P * Q] * U[i];
+    }
+    d_V[elem * strides_elem_V + comp * strides_comp_V + t_id] = r_V;
+  }
+}
+
+//------------------------------------------------------------------------------
+
+#endif  // CEED_HIP_REF_BASIS_NONTENSOR_TEMPLATES_H
diff --git a/include/ceed/jit-source/hip/hip-ref-basis-nontensor.h b/include/ceed/jit-source/hip/hip-ref-basis-nontensor.h
index 101f898e..fd389f8a 100644
--- a/include/ceed/jit-source/hip/hip-ref-basis-nontensor.h
+++ b/include/ceed/jit-source/hip/hip-ref-basis-nontensor.h
@@ -12,6 +12,8 @@
 
 #include <ceed.h>
 
+#include "hip-ref-basis-nontensor-templates.h"
+
 //------------------------------------------------------------------------------
 // Non-Tensor Basis Kernels
 //------------------------------------------------------------------------------
@@ -19,67 +21,38 @@
 //------------------------------------------------------------------------------
 // Interp
 //------------------------------------------------------------------------------
-extern "C" __global__ void Interp(const CeedInt num_elem, const CeedInt transpose, const CeedScalar *d_B, const CeedScalar *__restrict__ d_U,
+extern "C" __global__ void Interp(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
                                   CeedScalar *__restrict__ d_V) {
-  const CeedInt t_id = threadIdx.x;
-
-  const CeedScalar *U;
-  CeedScalar        V;
-  // TODO load B in shared memory if blockDim.z > 1?
-
   for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
-    for (CeedInt comp = 0; comp < BASIS_NUM_COMP; comp++) {
-      if (transpose) {  // run with P threads
-        U = d_U + elem * BASIS_Q + comp * num_elem * BASIS_Q;
-        V = 0.0;
-        for (CeedInt i = 0; i < BASIS_Q; i++) V += d_B[t_id + i * BASIS_P] * U[i];
-
-        d_V[elem * BASIS_P + comp * num_elem * BASIS_P + t_id] = V;
-      } else {  // run with Q threads
-        U = d_U + elem * BASIS_P + comp * num_elem * BASIS_P;
-        V = 0.0;
-        for (CeedInt i = 0; i < BASIS_P; i++) V += d_B[i + t_id * BASIS_P] * U[i];
+    Contract<BASIS_NUM_COMP, BASIS_Q_COMP_INTERP, BASIS_P, BASIS_Q>(elem, BASIS_P, BASIS_Q, BASIS_P * num_elem, BASIS_Q * num_elem,
+                                                                    BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
+  }
+}
 
-        d_V[elem * BASIS_Q + comp * num_elem * BASIS_Q + t_id] = V;
-      }
-    }
+extern "C" __global__ void InterpTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
+                                           CeedScalar *__restrict__ d_V) {
+  for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
+    ContractTranspose<BASIS_NUM_COMP, BASIS_Q_COMP_INTERP, BASIS_P, BASIS_Q>(elem, BASIS_Q, BASIS_P, BASIS_Q * num_elem, BASIS_P * num_elem,
+                                                                             BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
   }
 }
 
 //------------------------------------------------------------------------------
-// Grad
+// Deriv
 //------------------------------------------------------------------------------
-extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose, const CeedScalar *d_G, const CeedScalar *__restrict__ d_U,
-                                CeedScalar *__restrict__ d_V) {
-  const CeedInt t_id = threadIdx.x;
-
-  const CeedScalar *U;
-  // TODO load G in shared memory if blockDim.z > 1?
-
+extern "C" __global__ void Deriv(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
+                                 CeedScalar *__restrict__ d_V) {
   for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
-    for (CeedInt comp = 0; comp < BASIS_NUM_COMP; comp++) {
-      if (transpose) {  // run with P threads
-        CeedScalar V = 0.0;
-        for (CeedInt dim = 0; dim < BASIS_DIM; dim++) {
-          U = d_U + elem * BASIS_Q + comp * num_elem * BASIS_Q + dim * BASIS_NUM_COMP * num_elem * BASIS_Q;
-          for (CeedInt i = 0; i < BASIS_Q; i++) V += d_G[t_id + i * BASIS_P + dim * BASIS_P * BASIS_Q] * U[i];
-        }
-
-        d_V[elem * BASIS_P + comp * num_elem * BASIS_P + t_id] = V;
-      } else {  // run with Q threads
-        CeedScalar V[BASIS_DIM];
-        U = d_U + elem * BASIS_P + comp * num_elem * BASIS_P;
-        for (CeedInt dim = 0; dim < BASIS_DIM; dim++) V[dim] = 0.0;
-        for (CeedInt i = 0; i < BASIS_P; i++) {
-          const CeedScalar val = U[i];
-          for (CeedInt dim = 0; dim < BASIS_DIM; dim++) V[dim] += d_G[i + t_id * BASIS_P + dim * BASIS_P * BASIS_Q] * val;
-        }
+    Contract<BASIS_NUM_COMP, BASIS_Q_COMP_DERIV, BASIS_P, BASIS_Q>(elem, BASIS_P, BASIS_Q, BASIS_P * num_elem, BASIS_Q * num_elem,
+                                                                   BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
+  }
+}
 
-        for (CeedInt dim = 0; dim < BASIS_DIM; dim++) {
-          d_V[elem * BASIS_Q + comp * num_elem * BASIS_Q + dim * BASIS_NUM_COMP * num_elem * BASIS_Q + t_id] = V[dim];
-        }
-      }
-    }
+extern "C" __global__ void DerivTranspose(const CeedInt num_elem, const CeedScalar *__restrict__ d_B, const CeedScalar *__restrict__ d_U,
+                                          CeedScalar *__restrict__ d_V) {
+  for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
+    ContractTranspose<BASIS_NUM_COMP, BASIS_Q_COMP_DERIV, BASIS_P, BASIS_Q>(elem, BASIS_Q, BASIS_P, BASIS_Q * num_elem, BASIS_P * num_elem,
+                                                                            BASIS_NUM_COMP * BASIS_Q * num_elem, d_B, d_U, d_V);
   }
 }
 
@@ -89,6 +62,7 @@ extern "C" __global__ void Grad(const CeedInt num_elem, const CeedInt transpose,
 extern "C" __global__ void Weight(const CeedInt num_elem, const CeedScalar *__restrict__ q_weight, CeedScalar *__restrict__ d_V) {
   const CeedInt t_id = threadIdx.x;
   // TODO load q_weight in shared memory if blockDim.z > 1?
+
   for (CeedInt elem = blockIdx.x * blockDim.z + threadIdx.z; elem < num_elem; elem += gridDim.x * blockDim.z) {
     d_V[elem * BASIS_Q + t_id] = q_weight[t_id];
   }
diff --git a/interface/ceed-basis.c b/interface/ceed-basis.c
index 9e569584..1b86fa33 100644
--- a/interface/ceed-basis.c
+++ b/interface/ceed-basis.c
@@ -1208,7 +1208,7 @@ int CeedBasisCreateHcurl(Ceed ceed, CeedElemTopology topo, CeedInt num_comp, Cee
                          const CeedScalar *curl, const CeedScalar *q_ref, const CeedScalar *q_weight, CeedBasis *basis) {
   CeedInt Q = num_qpts, P = num_nodes, dim = 0, curl_comp = 0;
 
-  if (!ceed->BasisCreateHdiv) {
+  if (!ceed->BasisCreateHcurl) {
     Ceed delegate;
 
     CeedCall(CeedGetObjectDelegate(ceed, &delegate, "Basis"));
